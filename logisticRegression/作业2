1、逻辑回归是泛线性回归。
是将线性回归后的结果通过sigmoid函数映射到(0, 1)区间，进行二分类

2、3、原理及推导，见图

4、（1）正则化：在损失函数后面加正则项，用于限制模型过拟合
L1正则：参数w的绝对值的和，模型参数倾向于取0值
L2正则：参数w的欧氏距离，模型参数倾向于比较小的值

（2）模型评估：
分类模型：
1）误差率： 1/m * p(f(x) != y)
2）准确率： 1 - 误差率
3）精确率P:  TP / (TP + FP)
4）召回率R:  TP / (TP + FN)
5）如果输出一个概率，可以用P-R曲线，或者ROC来挑选模型

回归模型：
均方误差： 1/m * (y - f(x))^2

5、逻辑回归优缺点：
优点：输出(0, 1)概率，可用于分类，或者求(0, 1)间的回归值
缺点：如果某些自变量之间有大的相关性，则对模型影响很大，要修正
     因为sigmoid函数的性质，模型对中间某些值的变化非常敏感，对两端大部分值的变化非常小，
     不能反映大部分区间的变化

6、样本不均衡问题： 不同类别样本数量要大致相同，否则模型会倾向多数样本，出现偏差
（1）调整判断的阈值： p > m+ / m 为正样本
其中，p为输出的概率，m+ 为正样本个数， m 为样本总数
m+ / m 为随机模型输出正样本概率

（2）过采样，用SMOTE法生成（对少数样本中的任两个值，生成中间值，放回样本中，循环）

（3）欠采样。欠采样可以跟bagging结合，每次训练选少数样本，加一个多数样本的欠采样。
bagging：每次采样都是放回采样，不改变原来样本的概率分布



